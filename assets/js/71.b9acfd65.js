(window.webpackJsonp=window.webpackJsonp||[]).push([[71],{495:function(s,t,a){"use strict";a.r(t);var e=a(31),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("p",[s._v("经典的覆盖率导向fuzz,")]),s._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#运行策略-覆盖率导向"}},[s._v("运行策略: 覆盖率导向")]),a("ul",[a("li",[a("a",{attrs:{href:"#思路"}},[s._v("思路")])]),a("li",[a("a",{attrs:{href:"#路径信息的记录-标记路径"}},[s._v("路径信息的记录 - 标记路径")])]),a("li",[a("a",{attrs:{href:"#路径信息的分析"}},[s._v("路径信息的分析")])]),a("li",[a("a",{attrs:{href:"#fork-server"}},[s._v("fork server")])]),a("li",[a("a",{attrs:{href:"#执行测试"}},[s._v("执行测试")])])])]),a("li",[a("a",{attrs:{href:"#变异策略"}},[s._v("变异策略")]),a("ul",[a("li",[a("a",{attrs:{href:"#确定性变异"}},[s._v("确定性变异")])]),a("li",[a("a",{attrs:{href:"#随机性变异-havoc-大破坏"}},[s._v("随机性变异 havoc  大破坏")])]),a("li",[a("a",{attrs:{href:"#文件拼接-splice"}},[s._v("文件拼接 splice")])]),a("li",[a("a",{attrs:{href:"#cycle"}},[s._v("cycle")])])])]),a("li",[a("a",{attrs:{href:"#其他一些重要部分"}},[s._v("其他一些重要部分")]),a("ul",[a("li",[a("a",{attrs:{href:"#afl-fuzzer的目录结构"}},[s._v("afl fuzzer的目录结构:")])]),a("li",[a("a",{attrs:{href:"#sync-fuzzer-多fuzzer协作"}},[s._v("sync_fuzzer 多fuzzer协作")])]),a("li",[a("a",{attrs:{href:"#语料修剪-trim-case"}},[s._v("语料修剪 trim_case")])])])]),a("li",[a("a",{attrs:{href:"#参考"}},[s._v("参考")])])])]),a("p"),s._v(" "),a("h2",{attrs:{id:"运行策略-覆盖率导向"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#运行策略-覆盖率导向"}},[s._v("#")]),s._v(" 运行策略: 覆盖率导向")]),s._v(" "),a("h3",{attrs:{id:"思路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#思路"}},[s._v("#")]),s._v(" 思路")]),s._v(" "),a("p",[s._v("afl的使用，")]),s._v(" "),a("p",[s._v("首先设置编译器为"),a("code",[s._v("afl-gcc")]),s._v(" 进行编译，得到的文件使用afl-fuzz进行fuzz测试。")]),s._v(" "),a("p",[s._v("afl实现思路，")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("首先载入用户提供的测试用例初始化队列，")])]),s._v(" "),a("li",[a("p",[s._v("开始循环")]),s._v(" "),a("ul",[a("li",[s._v("然后从队列中获取下一个输入，")]),s._v(" "),a("li",[s._v("进行测试用例的修剪，  在不更改出程序行为的基础上，得到最小用例，")]),s._v(" "),a("li",[s._v("使用一系列的变异策略反复变异该文件并进行测试")]),s._v(" "),a("li",[s._v("如果存在一些新的路径产生了，那么讲该变异作为输入添加到队列中")])])])]),s._v(" "),a("h4",{attrs:{id:"代码插桩-标记基本块"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#代码插桩-标记基本块"}},[s._v("#")]),s._v(" 代码插桩 -标记基本块")]),s._v(" "),a("p",[s._v("对于路径的探索主要的一个点就是在路径变化的时候进行检测，这个检测的实现 就是插桩。")]),s._v(" "),a("p",[s._v("简单来说，就是对每个基本块做一个标记。")]),s._v(" "),a("p",[s._v("首先在使用"),a("code",[s._v("afl-gcc")]),s._v(" 只是一个对gcc的封装，进行编译的时候，我们可以通过打印参数看到这一层封装")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/YmLwW4IoCKBpcRi.png",alt:"image.png"}})]),s._v(" "),a("p",[s._v("重要的是增加了这个 "),a("code",[s._v("-B PATH")]),s._v("参数，会在gcc编译的时候从这个路径查找需要的文件并运行，也就是我们可以hook某个编译的过程，")]),s._v(" "),a("blockquote",[a("p",[s._v("gcc编译过程为，")]),s._v(" "),a("ul",[a("li",[s._v("预编译， 处理宏等")]),s._v(" "),a("li",[s._v("编译， 产生汇编代码")]),s._v(" "),a("li",[s._v("汇编， 产生字节码")]),s._v(" "),a("li",[s._v("链接， 生成可执行文件")])])]),s._v(" "),a("p",[s._v("当前目录下，存在一个as的链接文件，")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/uVPgMqafvRzGwmJ.png",alt:"image.png"}})]),s._v(" "),a("p",[s._v("于是这个"),a("code",[s._v("afl-gcc")]),s._v("会转入到这个"),a("code",[s._v("afl-as")]),s._v("进行运行，")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/IJHg5pxasyFQqSL.png",alt:"image.png"}})]),s._v(" "),a("p",[s._v("他的参数比较简单，就是封装了一层"),a("code",[s._v("as")]),s._v("， 但是在调用as之前， 对这一段汇编代码， 进行了插桩处理。")]),s._v(" "),a("p",[s._v("可以翻阅 "),a("code",[s._v("afl-as.c")]),s._v("中的"),a("code",[s._v("add_instrumentation")]),s._v("  函数。")]),s._v(" "),a("p",[s._v("基本就是， 在程序开始和程序跳转的位置插入"),a("code",[s._v("trampoline_fmt_64")]),s._v(" 并配合产生的随机数表示这个位置。")]),s._v(" "),a("p",[s._v("其实就是通过随机数对每个基本块进行标记，")]),s._v(" "),a("blockquote",[a("p",[s._v("这个其实也可以通过 llvm进行实现，在"),a("code",[s._v("afl/llvm_mode")]),s._v("中是llvm的版本，那个也比较简单 实现一个 pass就好。")]),s._v(" "),a("p",[s._v("使用gcc进行实现的话，确实只能如此。")])]),s._v(" "),a("p",[s._v("那么这个步骤插入的代码如下， 插桩前后对比。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/k9PCjWEpy5dJAwD.png",alt:"image.png"}})]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/RbX3kTfBJgFZcmL.png",alt:"image.png"}})]),s._v(" "),a("h3",{attrs:{id:"路径信息的记录-标记路径"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#路径信息的记录-标记路径"}},[s._v("#")]),s._v(" 路径信息的记录 - 标记路径")]),s._v(" "),a("p",[s._v("其实这个部分，就是在记录流程图中的路径，我们已经可以将基本块表示为对应的数据了，")]),s._v(" "),a("p",[s._v("但是在这里一定是通过 两个基本块来表示从prev -> current的路径，")]),s._v(" "),a("p",[s._v("于是对于路径的记录，afl使用一个 "),a("code",[s._v("trace_mem[prev_loc ^ current_loc]")]),s._v(",  表示代码在这个路径上运行的次数，这是一个类似哈希表的思路。")]),s._v(" "),a("p",[s._v("但是这里的"),a("code",[s._v("prev_loc")]),s._v("的更新， 为 "),a("code",[s._v("prev_loc = current_loc >> 1")]),s._v("， 这样可以避免 "),a("code",[s._v("^")]),s._v(" 运算符的运算交换率和运算表达式相同时结果相同的问题。")]),s._v(" "),a("blockquote",[a("p",[s._v("也就是我们的每一个记录的路径其实都是一个边， 从上个基本块到下个基本块的连接，因此要保证 a->b 和b->a，a->a和b->b不能相同，也就是我们说的交换率和运算表达式相同结果相同的问题。")])]),s._v(" "),a("p",[s._v("通过 "),a("code",[s._v("prev_loc")]),s._v("的更新和亦或运算，可以实现一个 可分辨前后基本块的哈兮算法， 用来表示一个有方向的路径，并如此实现一个哈希表用来表示， 每个路径运行了多少次。")]),s._v(" "),a("p",[s._v("这个"),a("code",[s._v("trace_mem")]),s._v(", 在afl fuzzer代码中是"),a("code",[s._v("trace_bits")]),s._v("， 在插桩的代码中为 "),a("code",[s._v("_afl_global_area_ptr")]),s._v("，")]),s._v(" "),a("p",[s._v("如此就完成了路径信息的记录。")]),s._v(" "),a("p",[s._v("这里其实是我们前面没考虑到的点，我们跟踪的路径信息全部都是来自测试进程，但是路径信息的分析和后续操作都是fuzzer主进程， 因此这里使用进程间通讯或进程间内存共享。afl中实现的做法就是进程间内存共享，使用linux下的 shm。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/k1FGbP4lC6AY9qJ.png",alt:"image.png"}})]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/hjkEqu1LGOrWTQv.png",alt:"image.png"}})]),s._v(" "),a("p",[s._v("以上的算法都在函数 "),a("code",[s._v("__afl_maybe_log")]),s._v(" 中， 每次运行到一个路径之前都会运行这个函数")]),s._v(" "),a("blockquote",[a("p",[s._v("前面的插桩就是对每个基本块插入了对这个函数的调用，")])]),s._v(" "),a("p",[s._v("这里其实还会面临一个问题，那就是使用hash表的话，肯定会存在一个碰撞概率，afl测试表示这种算法对于碰撞的概率不高， 可以比较稳定的使用。")]),s._v(" "),a("h3",{attrs:{id:"路径信息的分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#路径信息的分析"}},[s._v("#")]),s._v(" 路径信息的分析")]),s._v(" "),a("p",[s._v("先了解afl对于路径的判断，运行某个实例以后，afl如何知道触发了新路径呢？")]),s._v(" "),a("p",[s._v("因为已经通过内存共享同时处理了这块内存，并且通过跳转作为索引进行标记，那么可以直接比较或者hash以后进行比较，这个思路是很简洁流畅的。在具体实现中有一下函数：")]),s._v(" "),a("h4",{attrs:{id:"classify-count"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#classify-count"}},[s._v("#")]),s._v(" "),a("code",[s._v("classify_count")])]),s._v(" "),a("p",[s._v("我们的路径记录其实重点在于判断是否达到路径， 其次才是运行多少次，")]),s._v(" "),a("blockquote",[a("p",[s._v("比如说，某次循环，运行一次和两次，其实本身没有什么差别，我们的fuzz不应该认为是一个新的路径，")]),s._v(" "),a("p",[s._v("但是如果是运行一次和运行一万次，还是应该看作不同的，因为一万次甚至不停止 死循环可以判断为dos。")])]),s._v(" "),a("p",[s._v("于是afl会在进行计算之前进行一次运算，就将路径处理一下。")]),s._v(" "),a("p",[s._v("处理思路就是在 只记录可达与否和记录运行次数之前取一个妥协， 引入运行次数的范围，某个范围之内，算作同一个数据。范围定义如下")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("const")]),s._v(" u8 count_class_lookup8"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("31")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("127")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("p",[s._v("实现部分在函数"),a("code",[s._v("classify_counts")])]),s._v(" "),a("h4",{attrs:{id:"has-new-bits"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#has-new-bits"}},[s._v("#")]),s._v(" "),a("code",[s._v("has_new_bits")])]),s._v(" "),a("p",[s._v("测试当前的路径是否产生了新路径。")]),s._v(" "),a("p",[a("code",[s._v("virgin")]),s._v(" 一般是传入的"),a("code",[s._v("virgin_bits")]),s._v("， 这个是当前运行的路径表， 如果需要同步则设置"),a("code",[s._v("bitmap_changed")]),s._v("会和"),a("code",[s._v("out_dir/fuzz_bitmap")]),s._v("进行同步，")]),s._v(" "),a("p",[s._v("而"),a("code",[s._v("current")]),s._v("是指向"),a("code",[s._v("trace_bits")]),s._v("， 为刚刚运行过的路径，")]),s._v(" "),a("p",[s._v("这个vir初始化是在函数"),a("code",[s._v("setup_shm")]),s._v("， 将这个内存全部设置为0xff， 而且如果出发了新路径则会运行一句 "),a("code",[s._v("*virgin &= ~*current;")]),s._v(", 于是10翻转，")]),s._v(" "),a("ul",[a("li",[s._v("在current中使用1表示路径运行过1次，")]),s._v(" "),a("li",[s._v("在vir中使用0表示路径运行过1次，")])]),s._v(" "),a("blockquote",[a("p",[s._v("这种方式非常方便使用了同样的数据意义 表达了  trace_bit表示单次运行，而virgin_bit表示总的运行图，")])]),s._v(" "),a("p",[s._v("进行测试，首先判断  "),a("code",[s._v("current != 0")]),s._v(" 这个路径运行了, "),a("code",[s._v("current & virgin != 0")]),s._v("，命令中了某个路径或者同一个路径运行的次数不同，")]),s._v(" "),a("blockquote",[a("p",[s._v("这里需要了解到前面提到的三个点，")]),s._v(" "),a("ul",[a("li",[s._v("我们进行记录的信息，在程序内是通过"),a("code",[s._v("BYTE")]),s._v("形式写入的， 因此每个byte表示一条跳转，")]),s._v(" "),a("li",[s._v("我们获取到的路径图会通过"),a("code",[s._v("classify_count")]),s._v("函数进行简化，这个函数设置的几个值在byte范围之内其实都是互相 & 为0的，")]),s._v(" "),a("li",[s._v("这里的vir和current表示是01相反的，因此这个位置表示的意义是 命中了一个没有运行过的路径或者运行了一个路径没有运行过的次数。")])])]),s._v(" "),a("blockquote",[a("p",[s._v("这个就是我们提到的， "),a("code",[s._v("trace_bit")]),s._v(" 和 "),a("code",[s._v("virgin_bit")]),s._v("相反带来的妙用。")])]),s._v(" "),a("p",[s._v("接下来判断的是，"),a("code",[s._v("cur[0] && vir[0] == 0xff")]),s._v(" ， 将原本的u64分为8个u8进行比较，同时，这个位置的 "),a("code",[s._v("==")]),s._v(" 比 "),a("code",[s._v("&&")]),s._v("优先级高， 因此先判断后者，也就是这个判断是判断是否是曾经未运行到的新路径。")]),s._v(" "),a("p",[s._v("因此这个函数的逻辑也比较好理解了。")]),s._v(" "),a("ul",[a("li",[s._v("没有路径运行到或者是和上次相同那么返回0,")]),s._v(" "),a("li",[s._v("如果运行了某个路径的新的次数那么返回1,")]),s._v(" "),a("li",[s._v("如果运行了某个之前没有被运行的路径，那么返回2,  "),a("strong",[s._v("新路径")])])]),s._v(" "),a("h3",{attrs:{id:"fork-server"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fork-server"}},[s._v("#")]),s._v(" fork server")]),s._v(" "),a("p",[s._v("模糊测试这个事情，是在子进程按照传入的测试样例运行afl-gcc编译出来的目标程序， 而fuzz本身是一个父进程， 这里肯定是fork+exec产生的，这一点应该比较简单想到，")]),s._v(" "),a("p",[s._v("但是fuzz中需要不断的创建子进程， 这就导致需要不断的 frok + exec， 其实对于系统 fork一般会有写时复制的优化， 但是对于exec却没办法， 而且exec需要解析elf文件设置相关动态链接库是效率是难以容忍的，")]),s._v(" "),a("p",[s._v("于是afl设计了一个 fork server ， fuzzer进程和fork-server通过管道进行通讯，然后fork-server是已经execve以后的测试进程，这个进程一直在死循环中， 通过fuzzer的信号，不断进行fork和获取子进程状态并返回的操作，")]),s._v(" "),a("h4",{attrs:{id:"fuzzer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fuzzer"}},[s._v("#")]),s._v(" fuzzer")]),s._v(" "),a("p",[s._v("这一部分代码， 在fuzzer，初始化fork-server在函数"),a("code",[s._v("init_forkserver")]),s._v("， 新建测试进程端在函数 "),a("code",[s._v("run_target")])]),s._v(" "),a("p",[s._v("在"),a("code",[s._v("init_forkserver")]),s._v("函数中，最主要的两个点就是创建和设置管道，将 198和199文件描述符作为控制传入端和获取状态，然后就是exec,  然后通过管道数据传输获取到fork-server创建成功，后续就可以正常使用了")]),s._v(" "),a("p",[s._v("然后在"),a("code",[s._v("run_target")]),s._v("函数中，如果不存在fork_server的话，会进入常规的fork-exec的运行路线，")]),s._v(" "),a("p",[s._v("如果存在fork_server， 则直接向控制传入端"),a("code",[s._v("fsrv_ctl_fd")]),s._v(" 写入本次的时间(如果超时就会kill掉)， 并且在后面等待状态获取端"),a("code",[s._v("fsrv_st_fd")]),s._v("返回状态回来，后续判断本次运行的状态即可。")]),s._v(" "),a("h4",{attrs:{id:"fork-server-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fork-server-2"}},[s._v("#")]),s._v(" fork-server")]),s._v(" "),a("p",[s._v("我们再看fork-server这一边，")]),s._v(" "),a("p",[s._v("其实执行这么一套， 对于我们exec出来的fork-server来说，其实我们本身无法改变他的运行起点，肯定是main函数开始运行，这里afl的实现中非常巧妙的将这个fork-server和之前我们提到的"),a("code",[s._v("__afl_maybe_log")]),s._v(" 结合在一起，")]),s._v(" "),a("p",[s._v("在这个位置， 可以看到， 一旦进入这个判断，就会一直在这个里面进行死循环，除非发生错误， 这个位置所作的事情也就是我们所说的fork-server的工作，")]),s._v(" "),a("p",[s._v("而进入这个判断，就是从199文件描述符获取数据，在直接运行启动或者fuzzer进行fork-exec启动的时候， 大概率不会存在这么大的一个文件描述符， 因此只会运行常规的路径标记操作，")]),s._v(" "),a("p",[s._v("在fork-server产出新的进程后，也是进入"),a("code",[s._v("__afl_fork_resume")]),s._v(" 位置，关闭两个文件描述符，")]),s._v(" "),a("p",[s._v("因此能进入的只有fork-server， 因为只有这时候才会有这个文件描述符，而我们前面提到的主进程也是通过这个写入来判断是否成功创建了fork-server，")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/m6JHBtiyUlsgohY.png",alt:"image.png"}})]),s._v(" "),a("p",[a("img",{attrs:{src:"https://s2.loli.net/2022/04/03/CVlFuK9gtBh6PRz.png",alt:"image.png"}})]),s._v(" "),a("h3",{attrs:{id:"执行测试"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#执行测试"}},[s._v("#")]),s._v(" 执行测试")]),s._v(" "),a("h4",{attrs:{id:"common-fuzz-stuff"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#common-fuzz-stuff"}},[s._v("#")]),s._v(" "),a("code",[s._v("common_fuzz_stuff")])]),s._v(" "),a("p",[s._v("执行测试的函数为 "),a("code",[s._v("common_fuzz_stuff")]),s._v("， 被函数"),a("code",[s._v("fuzz_one")]),s._v("调用， 在每次变异以后都要进行调用，进行测试。")]),s._v(" "),a("p",[a("code",[s._v("common_fuzz_stuff")]),s._v("函数主要逻辑就是调用函数"),a("code",[s._v("write_to_testcase")]),s._v("写入测试文件，然后调用"),a("code",[s._v("run_target")]),s._v("函数运行测试，并返回运行结果，然后调用函数"),a("code",[s._v("save_if_interesting")]),s._v("函数判断是否触发新路径并进行保存。")]),s._v(" "),a("h4",{attrs:{id:"run-target"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#run-target"}},[s._v("#")]),s._v(" "),a("code",[s._v("run_target")])]),s._v(" "),a("p",[s._v("将整个 trace_bits清空， 然后运行程序，这里运行会使用两种方式，直接fork+execve或者使用fork-server， 这就是我们前面提到的方案的使用。")]),s._v(" "),a("p",[s._v("然后获取到程序的返回状态，并检测了程序运行时间，使用"),a("code",[s._v("classify_counts")]),s._v(" 对执行的路径进行了简单的分类，然后通过程序返回状态进行状态的返回， 返回以下四种情况：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Execution status fault codes */")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("enum")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 00 */")]),s._v(" FAULT_NONE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 01 */")]),s._v(" FAULT_TMOUT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 02 */")]),s._v(" FAULT_CRASH"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 03 */")]),s._v(" FAULT_ERROR"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 04 */")]),s._v(" FAULT_NOINST"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* 05 */")]),s._v(" FAULT_NOBITS\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("h4",{attrs:{id:"save-if-interesting"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save-if-interesting"}},[s._v("#")]),s._v(" "),a("code",[s._v("save_if_interesting")])]),s._v(" "),a("p",[s._v("首先调用"),a("code",[s._v("has_new_bits")]),s._v("判断是否有路径不同，如果不存在新路径则直接返回。")]),s._v(" "),a("blockquote",[a("p",[s._v("值得注意的是，这里路径没有新的话， 仍然会记录崩溃次数，但是不会进行保存，这也是我们在fuzzer中可以看到的，出现了很多crash但是保存的只有一部分的原因。")])]),s._v(" "),a("p",[s._v("然后继续的话，肯定是值得记录的crash了， 进行保存之前会调用"),a("code",[s._v("calibrate_case")]),s._v("进行一次校验， 返回结果只用来判断是否为"),a("code",[s._v("FAULT_ERROR")]),s._v("进行报错， 文件会按照对应的格式保存在"),a("code",[s._v("out_dir/queue")]),s._v("里面。")]),s._v(" "),a("p",[s._v("然后根据崩溃原因不同分别放入不同的文件夹内，")]),s._v(" "),a("ul",[a("li",[s._v("超时： 如果发现了新的超时路径则保存到"),a("code",[s._v("out_dir/hangs/")]),s._v(" ，如果"),a("code",[s._v("hang_tmout")]),s._v(" 比"),a("code",[s._v("exec_tmout")]),s._v("大的话，那么程序使用"),a("code",[s._v("hang_tmout")]),s._v("重新运行一遍， 如果得到结果是crash则进入crash分组，如果不是的话则返回。")]),s._v(" "),a("li",[s._v("crash：进行整理，判断是否触发新路径，如果可以的话加入到"),a("code",[s._v("out_dir/crash")]),s._v("目录。")])]),s._v(" "),a("h2",{attrs:{id:"变异策略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#变异策略"}},[s._v("#")]),s._v(" 变异策略")]),s._v(" "),a("p",[s._v("这里提到的代码都在"),a("code",[s._v("fuzz_one")]),s._v("函数内。")]),s._v(" "),a("h3",{attrs:{id:"确定性变异"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#确定性变异"}},[s._v("#")]),s._v(" 确定性变异")]),s._v(" "),a("p",[s._v("确定性变异是增加对应参数和主fuzz进程会运行的变异。")]),s._v(" "),a("h4",{attrs:{id:"bitflip"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bitflip"}},[s._v("#")]),s._v(" bitflip")]),s._v(" "),a("p",[s._v("就是字面意义， 字节翻转从0变成1 1变成0,")]),s._v(" "),a("p",[s._v("运行方案是，先翻转变异，然后运行测试，再翻转回来，也就是整个文件每次只会改变翻转的部分。")]),s._v(" "),a("p",[s._v("一共有六个阶段，")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("bitflip 1/1 每一个bit进行翻转，")]),s._v(" "),a("ul",[a("li",[s._v("这一步会尝试获取token")])])]),s._v(" "),a("li",[a("p",[s._v("bitflip 2/1 每两个bit进行翻转")])]),s._v(" "),a("li",[a("p",[s._v("bitflip 4/1 每四个bit进行翻转")])]),s._v(" "),a("li",[a("p",[s._v("bitflip 8/8  每次一个字节进行翻转")]),s._v(" "),a("ul",[a("li",[s._v("在进行字节翻转的时候，开始设置 effector map ， 记录该字节翻转是否有效 (是否可以产生新路径 ） 并且在之后的其他变异中都会参考 effector map来进行。")])])]),s._v(" "),a("li",[a("p",[s._v("bitflip 16/8 每次两个字节进行翻转")])]),s._v(" "),a("li",[a("p",[s._v("bitflip 32/8 每次四个字节进行翻转")])])]),s._v(" "),a("p",[s._v("比较重要的两个变异，bitflip 1/1和bitflip 8/8, 分别是bit翻转和byte翻转，")]),s._v(" "),a("h5",{attrs:{id:"token"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#token"}},[s._v("#")]),s._v(" token")]),s._v(" "),a("p",[s._v("在bit翻转时， 会尝试获取token,")]),s._v(" "),a("p",[s._v("具体是，在每次计算路径发现本次和这个测试样例原本的路径不同，会尝试记录对应的数据，当发现和原路径不同，但是连续几个是一样的路径， 那么afl会认为是一个token, 即某些标志位或者校验值，")]),s._v(" "),a("p",[s._v("注意这个检测的次数并不是bit， 而是byte ， 通过循环次数进行控制")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("!")]),s._v("dumb_mode "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("&&")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("stage_cur "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("&")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("并且为了控制token的大小和个数，分别有一下的宏定义。")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Length limits for auto-detected dictionary tokens: */")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token macro property"}},[a("span",{pre:!0,attrs:{class:"token directive-hash"}},[s._v("#")]),a("span",{pre:!0,attrs:{class:"token directive keyword"}},[s._v("define")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token macro-name"}},[s._v("MIN_AUTO_EXTRA")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")])])]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token macro property"}},[a("span",{pre:!0,attrs:{class:"token directive-hash"}},[s._v("#")]),a("span",{pre:!0,attrs:{class:"token directive keyword"}},[s._v("define")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token macro-name"}},[s._v("MAX_AUTO_EXTRA")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")])])]),s._v("\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("h5",{attrs:{id:"effector-map"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#effector-map"}},[s._v("#")]),s._v(" effector map")]),s._v(" "),a("p",[s._v("在进行byte翻转的时候，afl生成 effector map, 标记每个byte的翻转是否有效， 记为0 或 1,")]),s._v(" "),a("p",[s._v("这样的逻辑是， 如果一个byte完全翻转都不能带来路径改变， 那么afl认为这个byte为常规data, 而非某些重要的控制位 的metadata, 对变异来说意义不大， 后续跳过这些无效数据， 可以提高效率。")]),s._v(" "),a("p",[s._v("此外， 当文件有效byte超过"),a("code",[s._v("EFF_MAX_PERC")]),s._v("的时候，会直接将所有的数据全部设置为有效， 这个比例默认是 90%")]),s._v(" "),a("h4",{attrs:{id:"arithemetic"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arithemetic"}},[s._v("#")]),s._v(" arithemetic")]),s._v(" "),a("p",[s._v("arithemetic 也按照处理数据大小分为很多阶段，处理方式就是对数据进行加减操作，")]),s._v(" "),a("p",[s._v("简单的构架如下， 遍历整个输入文件，然后循环运行加减操作，这个加减的上限是"),a("code",[s._v("ARITH_MAX")]),s._v(" ， 默认值为35.")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("len"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" j"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" j"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("ARITH_MAX"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" j"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("此外， 通过 effector map和判断此次运算是否和bitflip变异结果相同，可以跳过一些运算，保证效率，")]),s._v(" "),a("p",[s._v("另外， 对于afl还会对大端序和小端序都进行变异。")]),s._v(" "),a("p",[s._v("阶段：")]),s._v(" "),a("ul",[a("li",[s._v("arith 8/8， 对于每个字节进行变异")]),s._v(" "),a("li",[s._v("arith 16/8, 对于每两个字节进行变异， 这里会进行大端序和小端序")]),s._v(" "),a("li",[s._v("arith 32/8， 对于没四个字节进行变异。")])]),s._v(" "),a("h4",{attrs:{id:"interest"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#interest"}},[s._v("#")]),s._v(" interest")]),s._v(" "),a("p",[s._v("数据替换，就是使用准备好的数据进行替换，")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token macro property"}},[a("span",{pre:!0,attrs:{class:"token directive-hash"}},[s._v("#")]),a("span",{pre:!0,attrs:{class:"token directive keyword"}},[s._v("define")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token macro-name"}},[s._v("INTERESTING_8")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 8-bit when decremented  */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("            ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/*                                         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("            ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/*                                         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("            ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/*                                         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("           ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("           ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("           ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("127")]),s._v("           ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 8-bit when incremented  */")])]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token macro property"}},[a("span",{pre:!0,attrs:{class:"token directive-hash"}},[s._v("#")]),a("span",{pre:!0,attrs:{class:"token directive keyword"}},[s._v("define")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token macro-name"}},[s._v("INTERESTING_16")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32768")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("        ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 16-bit when decremented */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("129")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 8-bit                   */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 8-bit                   */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow unsig 8-bit when incremented   */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow unsig 8-bit                    */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("512")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("          ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("         ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1024")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("         ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4096")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("         ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* One-off with common buffer size         */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32767")]),s._v("         ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 16-bit when incremented */")])]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token macro property"}},[a("span",{pre:!0,attrs:{class:"token directive-hash"}},[s._v("#")]),a("span",{pre:!0,attrs:{class:"token directive keyword"}},[s._v("define")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token macro-name"}},[s._v("INTERESTING_32")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2147483648LL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 32-bit when decremented */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100663046")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("    ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Large negative number (endian-agnostic) */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32769")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("        ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 16-bit                  */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32768")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("        ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 16-bit                  */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("65535")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("        ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow unsig 16-bit when incremented  */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("65536")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("        ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow unsig 16 bit                   */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100663045")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("    ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Large positive number (endian-agnostic) */")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token expression"}},[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2147483647")]),s._v("    ")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Overflow signed 32-bit when incremented */")])]),s._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* Interesting values, as per config.h */")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" s8  interesting_8"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v(" INTERESTING_8 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" s16 interesting_16"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v(" INTERESTING_8"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" INTERESTING_16 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" s32 interesting_32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v(" INTERESTING_8"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" INTERESTING_16"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" INTERESTING_32 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br")])]),a("p",[s._v("然后对运行的数据进行替换即可， 这些数据一般都是临界数据等，有较大概率可以出发整数溢出。")]),s._v(" "),a("p",[s._v("同样，和之前一样，会根据effector map和 判断bitflip arithemetic 是否可达来进行跳过，优化效率。")]),s._v(" "),a("p",[s._v("阶段仍然是，8/8 16/8 32/8")]),s._v(" "),a("h4",{attrs:{id:"dictionary-full"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dictionary-full"}},[s._v("#")]),s._v(" dictionary full")]),s._v(" "),a("p",[s._v("字典填充，")]),s._v(" "),a("ul",[a("li",[s._v("user extras over ， 从头来是， 将用户提供的token替换到源文件中，")]),s._v(" "),a("li",[s._v("user extras insert， 从头开始， 将用户提供的token插入到源文件中")]),s._v(" "),a("li",[s._v("auto extras over, 从头开始，将之前bitflip 过程中识别到的token 替换到源文件中去。")])]),s._v(" "),a("h5",{attrs:{id:"over-token替换"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#over-token替换"}},[s._v("#")]),s._v(" over  token替换")]),s._v(" "),a("p",[s._v("对于user extras over 和 auto extras over的实现确实是类似的， 他们的行都是替换token，")]),s._v(" "),a("p",[s._v("首先，对于out_buf， 使用memcpy从"),a("code",[s._v("out_buf+i")]),s._v("直接覆盖token进去，将token全部替换一遍以后，使用memcpy恢复out_buf， 并增加i, 这样在每个位置都覆盖了token,")]),s._v(" "),a("p",[s._v("另外要注意的是，这样插入token的方案，需要对token的长度进行一次排序，避免需要恢复的情况。")]),s._v(" "),a("p",[s._v("这种插入方案还存在放弃的情况，比如插入后的token和原数据相同，插入的数据量超过长度，插入的位置effector map不存在有效位， 都会放弃此token的插入。")]),s._v(" "),a("h5",{attrs:{id:"insert-token插入"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#insert-token插入"}},[s._v("#")]),s._v(" insert token插入")]),s._v(" "),a("p",[s._v("算法就是设置一个ex_tmp的buf, 然后从ex_tmp+i开始，先复制token, 然后复制out_buf+i， 测试后设置 ex_tmp[i] = out_buf[i]， 并增加i,  于是可以在每个位置都插入token进去，")]),s._v(" "),a("p",[s._v("比较重要的一点是，插入token的方案是增加了代码， 所以不需要考虑是否和之前重复以及effector map的问题。")]),s._v(" "),a("h3",{attrs:{id:"随机性变异-havoc-大破坏"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机性变异-havoc-大破坏"}},[s._v("#")]),s._v(" 随机性变异 havoc  大破坏")]),s._v(" "),a("p",[s._v("所有的fuzzer都会运行的变异。 具体的实现在一个巨大的switch中， 其中的case都是之前提到的变异算法， 但是其中的数据是随机化的， 而且这个switch的分发也是通过随机数进行的， 因此完全成为了一个随机性变异。")]),s._v(" "),a("p",[s._v("其中会发生的变异有16个， 如下：")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("bit翻转，")])]),s._v(" "),a("li",[a("p",[s._v("设置为随机的interesting value，")]),s._v(" "),a("ul",[a("li",[s._v("byte大小，")]),s._v(" "),a("li",[s._v("word大小， 随机选择大小端")]),s._v(" "),a("li",[s._v("dword大小， 随机选择大小端")])])]),s._v(" "),a("li",[a("p",[s._v("选择随机数据进行加法、进行减法，")]),s._v(" "),a("ul",[a("li",[s._v("byte")]),s._v(" "),a("li",[s._v("word， 随机大小端")]),s._v(" "),a("li",[s._v("dword, 随机大小端")])])]),s._v(" "),a("li",[a("p",[s._v("随机异或，")])]),s._v(" "),a("li",[a("p",[s._v("随机删除一段数据，")])]),s._v(" "),a("li",[a("p",[s._v("随机选择一个位置插入、覆盖(替换)一段随机长度的数据，")]),s._v(" "),a("ul",[a("li",[s._v("这里使用 random%4, 并判断是否为0, 表示一个25%的概率。并由此分出两种变异情况。")]),s._v(" "),a("li",[s._v("25%的情况设置为一个随机选择的数据。")]),s._v(" "),a("li",[s._v("75%的情况从原文中随机复制一段。")])])]),s._v(" "),a("li",[a("p",[s._v("随机选择位置插入、覆盖 token。")])])]),s._v(" "),a("h3",{attrs:{id:"文件拼接-splice"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文件拼接-splice"}},[s._v("#")]),s._v(" 文件拼接 splice")]),s._v(" "),a("p",[s._v("在文件列表中继续选择文件，并尝试和当前文件进行比对，如果差别比较多的话，会将两个文件进行拼接， 然后继续进入 havoc环节。")]),s._v(" "),a("h3",{attrs:{id:"cycle"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cycle"}},[s._v("#")]),s._v(" cycle")]),s._v(" "),a("p",[s._v("前面分析的变异策略都是在函数 "),a("code",[s._v("fuzz_one")]),s._v("函数中的，")]),s._v(" "),a("p",[s._v("关于cycle这一部分就是在main函数中了，  在 while(1) 循环中， 挨个选择队列中的文件进入 fuzz_one函数， 当选择完了， 就到了下一次循环的时候了。")]),s._v(" "),a("h2",{attrs:{id:"其他一些重要部分"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其他一些重要部分"}},[s._v("#")]),s._v(" 其他一些重要部分")]),s._v(" "),a("h3",{attrs:{id:"afl-fuzzer的目录结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#afl-fuzzer的目录结构"}},[s._v("#")]),s._v(" afl fuzzer的目录结构:")]),s._v(" "),a("p",[s._v("afl允许多个fuzz一起运行，使用参数 "),a("code",[s._v("-M")]),s._v(" 设置为主fuzzer, 使用参数 "),a("code",[s._v("-S")]),s._v(" 设置为从fuzzer, 并且后跟id号， 然后对应的fuzz会在 指定的"),a("code",[s._v("out")]),s._v("目录下创建对应的 "),a("code",[s._v("out/{fuzz_id}")]),s._v(" 目录， 并且后续需要用到的目录都是在这个目录内的子目录获取和使用的。在默认情况下如果没有指定，那么这个目录将会定名为"),a("code",[s._v("out/default")]),s._v(" 。")]),s._v(" "),a("p",[s._v("其中afl fuzzer使用的目录有：")]),s._v(" "),a("ul",[a("li",[a("code",[s._v("queue")]),s._v("目录，  fuzzer运行时候我们前面提到按轮次运行，就是这个队列文件为一轮。并且这里面的文件都是按照"),a("code",[s._v('id:%06u"')]),s._v("的开头保存。")]),s._v(" "),a("li",[a("code",[s._v("crashes")]),s._v("目录， 可以产生crash的输入文件会收集都在这里。")]),s._v(" "),a("li",[a("code",[s._v("hangs")]),s._v(" 目录")]),s._v(" "),a("li",[a("code",[s._v(".synced")]),s._v(" 目录，只有存在"),a("code",[s._v("sync_id")]),s._v("的时候也就是设置了"),a("code",[s._v("-S/-M")]),s._v("等信息以后才会设置， 这个目录就是用来收集其他fuzzer的文件的。")])]),s._v(" "),a("p",[s._v("在afl fuzzer的代码中， 目录相关的全局变量如下：")]),s._v(" "),a("ul",[a("li",[a("p",[a("code",[s._v("in_dir")]),s._v("， 表示输入样例存放的文件夹，我们在命令行参数 -i 指定的。")])]),s._v(" "),a("li",[a("p",[a("code",[s._v("out_dir")]),s._v("， 表示输出文件夹，我们在命令行通过 -o 参数指定的。")])]),s._v(" "),a("li",[a("p",[a("code",[s._v("sync_dir")]),s._v(" ， 表示同步文件夹，")]),s._v(" "),a("ul",[a("li",[s._v("如果指定了sync_id的话，才会使用这个文件夹，用于多个fuzzer共享，")]),s._v(" "),a("li",[s._v("并且这时会调用函数 "),a("code",[s._v("fix_up_sync")]),s._v("， 设置"),a("code",[s._v("sync_dir=outdir")]),s._v(", "),a("code",[s._v("out_dir = out_dir/sync_id")]),s._v("，")])])])]),s._v(" "),a("h3",{attrs:{id:"sync-fuzzer-多fuzzer协作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sync-fuzzer-多fuzzer协作"}},[s._v("#")]),s._v(" "),a("code",[s._v("sync_fuzzer")]),s._v(" 多fuzzer协作")]),s._v(" "),a("p",[s._v("这个函数的作用就是获取其他fuzzer的case, 寻找有趣的一些case并尝试运行。 了解了fuzzer的目录结构就好理解多了。")]),s._v(" "),a("p",[s._v("首先打开"),a("code",[s._v("sync_dir")]),s._v("文件夹，循环读取其中的文件，获取文件名，如果不是"),a("code",[s._v(".")]),s._v("开头 并和我们的"),a("code",[s._v("sync_id")]),s._v("相同的(我们自身的"),a("code",[s._v("out_dir")]),s._v(")的话， 也就是其他的fuzzer的"),a("code",[s._v("out_dir")]),s._v("， 那么打开它，并且读取对应的"),a("code",[s._v("queue")]),s._v("文件夹。")]),s._v(" "),a("p",[s._v("如果内存不大的话， 会导入到内存中，然后通过"),a("code",[s._v("run_target")]),s._v("函数运行，并通过"),a("code",[s._v("save_if_interesting")]),s._v("函数决定是否保存，")]),s._v(" "),a("p",[s._v("因为fuzzer中的文件也按照id保存，因此还会维护"),a("code",[s._v("output/.synced")]),s._v("文件夹，里面对应fuzzer的sync_id为文件名的文件，其内容保存在对应的fuzzer中测试的最后一个文件的id, 这样可以保证不重复运行。")]),s._v(" "),a("h3",{attrs:{id:"语料修剪-trim-case"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#语料修剪-trim-case"}},[s._v("#")]),s._v(" 语料修剪 "),a("code",[s._v("trim_case")])]),s._v(" "),a("p",[s._v("语料修剪在每次"),a("code",[s._v("fuzz_one")]),s._v("函数开始时运行，进行修剪，得到可保持原执行状态的最小文件，然后再开始进行变异。")]),s._v(" "),a("p",[s._v("其实思路也还比较简单，就是直接跑，然后一直尝试修剪并运行，最后得到一个可以保证正常运行得到路径不变的最小长度，")]),s._v(" "),a("p",[s._v("通过 "),a("code",[s._v("run_target")]),s._v("函数运行测试样例， 然后通过"),a("code",[s._v("hash32")]),s._v(" 得到运行路径的哈兮，用于判断是否路径变化，")]),s._v(" "),a("p",[s._v("如果确实存在更小的路径会设置"),a("code",[s._v("needs_write")]),s._v("标志，在后续进行文件重新写入。")]),s._v(" "),a("h2",{attrs:{id:"参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[s._v("#")]),s._v(" 参考")]),s._v(" "),a("p",[a("a",{attrs:{href:"http://rk700.github.io/2018/01/04/afl-mutations/",target:"_blank",rel:"noopener noreferrer"}},[s._v("文件变异"),a("OutboundLink")],1)]),s._v(" "),a("p",[a("a",{attrs:{href:"https://eternalsakura13.com/2020/08/23/afl/",target:"_blank",rel:"noopener noreferrer"}},[s._v("源码注释-sakura"),a("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=n.exports}}]);